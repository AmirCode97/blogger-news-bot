# Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒØŒ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø­Ø°Ù Ø§Ø®Ø¨Ø§Ø± Ù‚Ø¯ÛŒÙ…ÛŒ
import sys
sys.path.insert(0, '.')

from blogger_poster import BloggerPoster
from news_fetcher import NewsFetcher
from datetime import datetime, timedelta
import json
import os
import hashlib
from difflib import SequenceMatcher

BLOG_ID = '1276802394255833723'
CACHE_FILE = "news_cache.json"

def similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

def generate_news_id(title, link):
    """Generate unique ID for news item (same logic as NewsFetcher)"""
    unique_string = f"{title}_{link}"
    return hashlib.md5(unique_string.encode()).hexdigest()

def main():
    print("=" * 60)
    print("ğŸ§¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙˆØ¨Ù„Ø§Ú¯ Ùˆ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡")
    print("=" * 60)

    poster = BloggerPoster()
    fetcher = NewsFetcher()
    
    # 1. Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ù‡ Ù¾Ø³Øªâ€ŒÙ‡Ø§
    all_posts = []
    page_token = None
    print("ğŸ“¥ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ ÙˆØ¨Ù„Ø§Ú¯...")
    while True:
        try:
            result = poster.service.posts().list(
                blogId=BLOG_ID, 
                maxResults=50, 
                pageToken=page_token,
                fetchBodies=False  # ÙÙ‚Ø· Ø¹Ù†ÙˆØ§Ù† Ùˆ ØªØ§Ø±ÛŒØ® Ú©Ø§ÙÛŒ Ø§Ø³Øª (Ø³Ø±ÛŒØ¹ØªØ±)
            ).execute()
            all_posts.extend(result.get('items', []))
            page_token = result.get('nextPageToken')
            if not page_token:
                break
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø³Øªâ€ŒÙ‡Ø§: {e}")
            break
            
    print(f"   Ú©Ù„ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {len(all_posts)}")
    
    # 2. Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒÙ‡Ø§
    seen_titles = []
    posts_to_delete = []
    seen_ids_to_add = set()
    
    # ØªØ§Ø±ÛŒØ® Ø§Ù…Ø±ÙˆØ²
    now = datetime.now()
    
    # Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ØŒ Ù„ÛŒØ³Øª Ø±Ø§ Ø¨Ø±Ø¹Ú©Ø³ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ÛŒØ§ Ù…Ø±ØªØ¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŸ
    # Ù¾Ø³Øªâ€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù‚Ø¯ÛŒÙ… Ù…ÛŒâ€ŒØ¢ÛŒÙ†Ø¯.
    # Ù¾Ø³ Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒÙ… Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ØŒ Ø¨Ø¹Ø¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
    
    keep_count = 0
    
    for post in all_posts:
        title = post['title']
        post_id = post['id']
        url = post.get('url', '') # Ù„ÛŒÙ†Ú© Ù¾Ø³Øª Ø¯Ø± Ø¨Ù„Ø§Ú¯Ø± (Ù†Ù‡ Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±)
        
        # ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø± Ù¾Ø³Øª
        published_str = post['published'] # 2026-02-03T...
        try:
            published_dt = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
            # Ú†ÙˆÙ† timezone Ù‡Ø§ Ù…ØªÙØ§ÙˆØª Ø§Ø³ØªØŒ simple comparison
            published_dt = published_dt.replace(tzinfo=None)
        except:
            published_dt = now
            
        is_duplicate = False
        is_old = False
        
        # Ø§Ù„Ù) Ú†Ú© Ú©Ø±Ø¯Ù† ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø¹Ù†ÙˆØ§Ù† (Ø¨Ø§ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§Ù„Ø§)
        for seen_t in seen_titles:
            if similarity(title, seen_t) > 0.9: # 90% Ø´Ø¨Ø§Ù‡Øª
                is_duplicate = True
                break
        
        # Ø¨) Ú†Ú© Ú©Ø±Ø¯Ù† Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨ÙˆØ¯Ù† (Ù…Ø«Ù„Ø§Ù‹ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ø§Ø² Û· Ø±ÙˆØ²ØŸ)
        # Ú©Ø§Ø±Ø¨Ø± Ú¯ÙØª "Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø±Ø§ Ù¾Ø§Ú© Ú©Ù†"
        # Ø¨ÛŒØ§ÛŒÛŒØ¯ ÙØ±Ø¶ Ú©Ù†ÛŒÙ… Ø§Ø®Ø¨Ø§Ø± Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ø§Ø² Ûµ Ø±ÙˆØ² Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯.
        if (now - published_dt).days > 5:
            is_old = True
            
        if is_duplicate:
            print(f"   ğŸ—‘ï¸ ØªÚ©Ø±Ø§Ø±ÛŒ: {title[:50]}...")
            posts_to_delete.append(post)
        elif is_old:
            print(f"   ğŸ•°ï¸ Ù‚Ø¯ÛŒÙ…ÛŒ ({published_dt.date()}): {title[:50]}...")
            posts_to_delete.append(post)
        else:
            # Ù¾Ø³Øª Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ -> Ø¨Ù‡ seen_titles Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
            seen_titles.append(title)
            keep_count += 1
            
            # ØªÙˆÙ„ÛŒØ¯ ID Ø¨Ø±Ø§ÛŒ cache (Ú©Ù‡ Ù…Ø¨Ø§Ø¯Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ÙÚ† Ø´ÙˆØ¯)
            # Ú†ÙˆÙ† Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø±Ø§ Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ ÛŒÚ© Ù‡Ø´ Ø§Ø² ØªØ§ÛŒØªÙ„ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ… Ú©Ù‡ NewsFetcher Ù‡Ù… Ú†Ú© Ú©Ù†Ø¯
            # Ø§Ù…Ø§ NewsFetcher Ø§Ø² (Title + Link) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            # Ù…Ø§ ÙÙ‚Ø· Title Ø±Ø§ Ø¯Ø§Ø±ÛŒÙ…. 
            # Ù¾Ø³ Ø¨Ø§ÛŒØ¯ NewsFetcher Ø±Ø§ Ø·ÙˆØ±ÛŒ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒÙ… Ú©Ù‡ Ø§Ú¯Ø± Title ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯ Ù‡Ù… Ù†Ú¯ÛŒØ±Ø¯.
            pass

    print(f"\nğŸ“Š ÙˆØ¶Ø¹ÛŒØª:")
    print(f"   âœ… Ø³Ø§Ù„Ù… Ùˆ Ø¬Ø¯ÛŒØ¯: {keep_count}")
    print(f"   âŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù: {len(posts_to_delete)}")
    
    # 3. Ø­Ø°Ù Ù¾Ø³Øªâ€ŒÙ‡Ø§
    if posts_to_delete:
        print("\nğŸ—‘ï¸ Ø´Ø±ÙˆØ¹ Ø­Ø°Ù...")
        for post in posts_to_delete:
            try:
                poster.service.posts().delete(blogId=BLOG_ID, postId=post['id']).execute()
                print(f"   Deleted: {post['title'][:30]}...")
            except Exception as e:
                print(f"   Failed to delete {post['id']}: {e}")
    else:
        print("\nâœ… Ù‡ÛŒÚ† Ù¾Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ù†ÛŒØ³Øª.")

    # 4. Ø¢Ù¾Ø¯ÛŒØª Ú©Ø±Ø¯Ù† news_cache.json
    # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø±Ø³Ø§Ù„ Ù…Ø¬Ø¯Ø¯ Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ (Ø­ØªÛŒ Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø±Ø§ Ù†Ø¯Ø§Ø±ÛŒÙ…)
    # ØªØ±ÙÙ†Ø¯: Ù…Ø§ Ù†Ù…ÛŒØªÙˆØ§Ù†ÛŒÙ… seen_ids Ø¯Ù‚ÛŒÙ‚ Ø¨Ø³Ø§Ø²ÛŒÙ… Ú†ÙˆÙ† Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø¯Ø³Øª Ù…Ø§ Ù†ÛŒØ³Øª.
    # Ø§Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÛŒÚ© ÙØ§ÛŒÙ„ `seen_titles.json` Ø¨Ø³Ø§Ø²ÛŒÙ… Ùˆ NewsFetcher Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒÙ… Ú©Ù‡ Ø¢Ù† Ø±Ø§ Ù‡Ù… Ú†Ú© Ú©Ù†Ø¯.
    
    print("\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø¯Ø± Ú©Ø´...")
    cache_data = {'seen_ids': list(fetcher.seen_news), 'seen_titles': seen_titles}
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ú©Ø´ ÙØ¹Ù„ÛŒ Ø§Ú¯Ø± Ù‡Ø³Øª
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
                # Ø§Ø¯ØºØ§Ù… ID Ù‡Ø§
                existing_ids = set(old_data.get('seen_ids', []))
                existing_ids.update(fetcher.seen_news)
                cache_data['seen_ids'] = list(existing_ids)
        except:
            pass
            
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False)
        
    print("âœ… Ú©Ø´ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯.")

if __name__ == "__main__":
    main()
