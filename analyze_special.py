# Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø®Ø´ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§ÛŒÙ†ØªØ±Ù†Ø´Ù†Ø§Ù„
from bs4 import BeautifulSoup
import sys
sys.path.insert(0, '.')
from news_fetcher import NewsFetcher

def analyze_special_reports():
    urls_to_test = [
        "https://www.iranintl.com/investigatives",
        "https://www.iranintl.com/program/investigatives",
        "https://www.iranintl.com/comments"
    ]
    
    fetcher = NewsFetcher()
    
    for url in urls_to_test:
        print(f"\nğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø¯Ø±Ø³: {url}")
        try:
             # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù‚Ù„ Ù‚ÙˆÙ„ Ø¨Ø±Ø§ÛŒ URL Ù‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
            from urllib.parse import quote
            if 'Ú¯Ø²Ø§Ø±Ø´' in url:
                 parts = url.split('tag/')
                 encoded_tag = quote(parts[1])
                 url = f"{parts[0]}tag/{encoded_tag}"
                 print(f"   (Encoded URL: {url})")

            response = fetcher._make_request(url, use_proxy=True)
            if not response:
                print("   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„.")
                continue

            soup = BeautifulSoup(response.content, 'html.parser')
            links = soup.select("a[href*='/202']")
            print(f"   ğŸ“„ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú© Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡: {len(links)}")
            
            if len(links) > 0:
                print("   âœ… Ø§ÛŒÙ† Ø¢Ø¯Ø±Ø³ Ù…Ø¹ØªØ¨Ø± Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯!")
                # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡
                for link in links[:3]:
                    print(f"      - {link.get_text().strip()} ({link.get('href')})")
        
        except Exception as e:
            print(f"   âŒ Ø®Ø·Ø§: {e}")

if __name__ == "__main__":
    analyze_special_reports()
